{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Support Vector Machines\n",
    "Support Veector machines are a classification method where we try to draw a line that divides two regions by the largest margin possible. The simplified mathematics behind this goes as follows:\n",
    "\n",
    "We start off with a statement, calculating the label of two given points.\n",
    "\n",
    "$$ w_0 + \\boldsymbol{w}^T\\boldsymbol{x}_{pos} = 1 \\\\ w_0 + \\boldsymbol{w}^T\\boldsymbol{x}_{pos} = -1 $$\n",
    "\n",
    "Technically, these two equations can describe hiperplanes that are parallel to the decision boundary. We want the distance between these two hyperplanes to be as large as possible. To accomplish this, we combine the two by subtracting the latter from the former to create\n",
    "\n",
    "$$ w_0 + \\boldsymbol w^T \\boldsymbol x_{pos} - w_o - \\boldsymbol w^T \\boldsymbol x_{neg} = \\boldsymbol w^T(\\boldsymbol x_{pos} - \\boldsymbol x_{neg}) = 2 $$\n",
    "\n",
    "To normalize this equation, we divide both sides by the by the $l_2$ norm\n",
    "\n",
    "$$ ||\\boldsymbol w|| = \\sqrt{\\sum_{j=1}^m w^2_j } $$\n",
    "\n",
    "to get\n",
    "\n",
    "$$ \\frac{\\boldsymbol w^T(\\boldsymbol x_{pos} - \\boldsymbol x_{neg})}{||\\boldsymbol w ||} = \\frac2{||\\boldsymbol w ||} $$\n",
    "\n",
    "We can now use a variety of methods to maximize the value on the RHS of the above equation to maximize the distance between the two hyperplanes mentioned above under the constraint that we still classify everything correctly which can be most consisly expressed as\n",
    "\n",
    "$$ y^{(i)} (w_0 + \\boldsymbol w^T \\boldsymbol x^{(i)} ) \\ge 1 \\space\\space \\forall_i $$\n",
    "\n",
    "which basically says that the \"label\" on each point should match the sign of each classification value.\n",
    "\n",
    "## Non-linearly seperable cases\n",
    "In non-linearly seperable datasets, we can introduce (what I am going to call) a fudge factor to the constraints. If we seperate out the consiscely written version above, we can write this fudge factor into our conditions as\n",
    "\n",
    "$$ w_0 + \\boldsymbol w^T \\boldsymbol x^{(i)} \\ge 1 - \\xi^{(i)} \\text{ if } y^{(i)} = 1 \\\\ w_0 + \\boldsymbol w^T \\boldsymbol x^{(i)} \\le -1 + \\xi^{(i)} \\text{ if } y^{(i)} = -1  $$\n",
    "\n",
    "for $i = 1\\dots N$. This fudge factor introduces itself into the maximization term (which we have actually turned into a minimization) as\n",
    "\n",
    "$$ \\frac12 ||\\boldsymbol w ||^2 + C \\left(\\sum_i\\xi^{(i)}\\right)$$\n",
    "\n",
    "Here, $C$ can be treated as a hyperparameter to adjust how much tolerance we will allow in misclassification.\n",
    "\n",
    "Now, to actually do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-68d2ed94ca5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "svm.fit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
